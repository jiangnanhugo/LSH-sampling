# LSH-sampling

this code mantain the LSH-based self-attention algorithm. We propose a more efficient LSH scheme for the method, so that the running time complexity of $O(N\log N)$ could be reduce to $sublinear of N * \log N$.

we have implemented the following code:
- reformer LSH self attention
- our sampling-based LSH scheme.
- full self-attention
